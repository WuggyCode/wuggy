<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>wuggy.evaluators.ld1nn API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>wuggy.evaluators.ld1nn</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from math import exp

import statsmodels.api as sm
from Levenshtein import distance


def ld1nn(word_sample: [str],
          nonword_sample: [str],
          word_as_reference_level=True):
    &#34;&#34;&#34;
    Implementation of the LD1NN algorithm, used to automatically detect bias in pseudowords.

    For an experiment containing a number of stimuli, the algorithm performs the following:
        1. Compute the Levenshtein distances between the currently presented stimulus and all previously presented stimuli.
        2. Identify the previously presented stimuli that are at the k nearest distances from the current
        stimulus.
        3. Compute the probability of a word response for the given stimulus based on the relative frequency of words among the nearest neighbors.

    For more information about LD1NN, see DOI: 10.1075/ml.6.1.02keu

    Parameters:
        word_sample: a list of real words. Make sure this list contains at least all words which all unique words in nonword_sample were based on. This list must contain the same amount of items as nonword_sample.

        nonword_sample: a list of nonwords words. This list must contain the same amount of items as word_sample.

        word_as_reference_level: set the word as reference level. If set to true, the odds returned by LD1NN represent how much likelier it is for a stimulus predicted as a word to be a word than a stimulus with a nonword prediction. If set to true, the vice versa is calculated.
    .. include:: ../../documentation/evaluators/ld1nn.md
    &#34;&#34;&#34;
    # TODO: implement a parallel processing option

    if (len(word_sample) != len(nonword_sample)):
        raise ValueError(&#34;Both sample lists need to contain the same amount of strings.&#34;)

    def get_probability(index: int):
        samples_with_distance = []
        for word in sample[0:index]:
            samples_with_distance.append((word[0], word[1], distance(word[0], sample[index][0])))
        samples_with_distance.sort(key=lambda value: value[2])
        minimum_distance = samples_with_distance[0][2]
        distribution = [sample for sample in samples_with_distance if sample[2] &lt;= minimum_distance]
        reference_level = &#34;word&#34; if word_as_reference_level else &#34;nonword&#34;
        probability = len([sample for sample in distribution if sample[1]
                           == reference_level]) / len(distribution)

        return probability

    sample = []
    for word in word_sample:
        sample.append((word, &#34;word&#34;))
    for word in nonword_sample:
        sample.append((word, &#34;nonword&#34;))

    index = 1
    # Start from the second word
    probabilities = [0.5]

    for word in sample[1::]:
        probabilities.append(get_probability(index))
        index += 1

    if word_as_reference_level:
        probabilities = list(map(lambda x: x*-1, probabilities))

    model_data = {&#34;probabilities&#34;: probabilities, &#34;types&#34;: [word[1] for word in sample]}
    fit = sm.formula.glm(
        &#34;types~(-1+probabilities)&#34;,
        family=sm.families.Binomial(), data=model_data).fit()
    return {&#34;odds&#34;: exp(fit.params[0]), &#34;standard_error&#34;: fit.tvalues[0], &#34;P&gt;|z|&#34;: fit.pvalues[0]}</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="wuggy.evaluators.ld1nn.ld1nn"><code class="name flex">
<span>def <span class="ident">ld1nn</span></span>(<span>word_sample: [<class 'str'>], nonword_sample: [<class 'str'>], word_as_reference_level=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Implementation of the LD1NN algorithm, used to automatically detect bias in pseudowords.</p>
<p>For an experiment containing a number of stimuli, the algorithm performs the following:
1. Compute the Levenshtein distances between the currently presented stimulus and all previously presented stimuli.
2. Identify the previously presented stimuli that are at the k nearest distances from the current
stimulus.
3. Compute the probability of a word response for the given stimulus based on the relative frequency of words among the nearest neighbors.</p>
<p>For more information about LD1NN, see DOI: 10.1075/ml.6.1.02keu</p>
<h2 id="parameters">Parameters</h2>
<p>word_sample: a list of real words. Make sure this list contains at least all words which all unique words in nonword_sample were based on. This list must contain the same amount of items as nonword_sample.</p>
<p>nonword_sample: a list of nonwords words. This list must contain the same amount of items as word_sample.</p>
<p>word_as_reference_level: set the word as reference level. If set to true, the odds returned by LD1NN represent how much likelier it is for a stimulus predicted as a word to be a word than a stimulus with a nonword prediction. If set to true, the vice versa is calculated.</p>
<h1 id="ld1nn-examples">LD1NN Examples</h1>
<h2 id="evaluating-wuggy-pseudowords">Evaluating Wuggy Pseudowords</h2>
<p>LD1NN is perfectly suitable for evaluating pseudowords generated by Wuggy. The following code snippet describes a fictional experiment in which we want to create ten pseudowords for ten real English words and evaluate them using LD1NN to see if we find a word bias.</p>
<pre><code class="language-python">from wuggy import WuggyGenerator, ld1nn

g = WuggyGenerator()
g.load(&quot;orthographic_english&quot;)
words = [&quot;rats&quot;, &quot;rave&quot;, &quot;rays&quot;, &quot;raze&quot;, &quot;read&quot;, &quot;real&quot;, &quot;ream&quot;, &quot;reap&quot;, &quot;rear&quot;, &quot;road&quot;]
pseudowords = []
for w in g.generate_classic(words, ncandidates_per_sequence=1):
    pseudowords.append(w[&quot;pseudoword&quot;])

print(f&quot;Pseudowords used: {pseudowords}&quot;)
print(ld1nn(words, pseudowords))
</code></pre>
<p>Below is an example result from the script:</p>
<p><code>Pseudowords used: ['sats', 'rane', 'tays', 'rask', 'sead', 'reot', 'reem', 'seap', 'reer', 'roud']
{'odds': 1.0838804106503954, 'standard_error': 0.16781712105679011, 'P&gt;|z|': 0.8667271528642492}</code></p>
<p>In this case, the word bias is low: 1.08 is very close to 1 (where 1 indicates no word bias). However, due to the small sample size this result is insignificant as the P value is at 0.8. Of course, a real experiment relies on larger samples where a significant result is more realistic. This is also where LD1NN becomes the most useful. For large samples, it becomes extremely difficult to manually assess word bias. LD1NN can help by providing a fast way to determine word bias, even for very large samples.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ld1nn(word_sample: [str],
          nonword_sample: [str],
          word_as_reference_level=True):
    &#34;&#34;&#34;
    Implementation of the LD1NN algorithm, used to automatically detect bias in pseudowords.

    For an experiment containing a number of stimuli, the algorithm performs the following:
        1. Compute the Levenshtein distances between the currently presented stimulus and all previously presented stimuli.
        2. Identify the previously presented stimuli that are at the k nearest distances from the current
        stimulus.
        3. Compute the probability of a word response for the given stimulus based on the relative frequency of words among the nearest neighbors.

    For more information about LD1NN, see DOI: 10.1075/ml.6.1.02keu

    Parameters:
        word_sample: a list of real words. Make sure this list contains at least all words which all unique words in nonword_sample were based on. This list must contain the same amount of items as nonword_sample.

        nonword_sample: a list of nonwords words. This list must contain the same amount of items as word_sample.

        word_as_reference_level: set the word as reference level. If set to true, the odds returned by LD1NN represent how much likelier it is for a stimulus predicted as a word to be a word than a stimulus with a nonword prediction. If set to true, the vice versa is calculated.
    .. include:: ../../documentation/evaluators/ld1nn.md
    &#34;&#34;&#34;
    # TODO: implement a parallel processing option

    if (len(word_sample) != len(nonword_sample)):
        raise ValueError(&#34;Both sample lists need to contain the same amount of strings.&#34;)

    def get_probability(index: int):
        samples_with_distance = []
        for word in sample[0:index]:
            samples_with_distance.append((word[0], word[1], distance(word[0], sample[index][0])))
        samples_with_distance.sort(key=lambda value: value[2])
        minimum_distance = samples_with_distance[0][2]
        distribution = [sample for sample in samples_with_distance if sample[2] &lt;= minimum_distance]
        reference_level = &#34;word&#34; if word_as_reference_level else &#34;nonword&#34;
        probability = len([sample for sample in distribution if sample[1]
                           == reference_level]) / len(distribution)

        return probability

    sample = []
    for word in word_sample:
        sample.append((word, &#34;word&#34;))
    for word in nonword_sample:
        sample.append((word, &#34;nonword&#34;))

    index = 1
    # Start from the second word
    probabilities = [0.5]

    for word in sample[1::]:
        probabilities.append(get_probability(index))
        index += 1

    if word_as_reference_level:
        probabilities = list(map(lambda x: x*-1, probabilities))

    model_data = {&#34;probabilities&#34;: probabilities, &#34;types&#34;: [word[1] for word in sample]}
    fit = sm.formula.glm(
        &#34;types~(-1+probabilities)&#34;,
        family=sm.families.Binomial(), data=model_data).fit()
    return {&#34;odds&#34;: exp(fit.params[0]), &#34;standard_error&#34;: fit.tvalues[0], &#34;P&gt;|z|&#34;: fit.pvalues[0]}</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<header>
<h2><a href="/wuggy">Wuggy Documentation</a></h2>
<a class="homelink" rel="home" title="Wuggy Home" href="https://github.com/WuggyCode/wuggy/">
<img src="https://raw.githubusercontent.com/WuggyCode/wuggy/master/assets/wuggyIcon.jpg" width="35%" alt="Wuggy Logo">
</a>
<h1>Quickstarts</h1>
<p><a href="/wuggy/generators/wuggygenerator.html#generate-classic-examples" title="generate-pseudowords">Easily generate pseudowords</a></p>
<p><a href="/wuggy/plugins/baselanguageplugin.html#creating-a-custom-language-plugin" title="create-language-plugin">Create a custom language plugin</a></p>
<p><a href="/wuggy/evaluators/index.html" title="evaluate-pseudowords">Evaluate pseudowords</a></p>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="wuggy.evaluators" href="index.html">wuggy.evaluators</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="wuggy.evaluators.ld1nn.ld1nn" href="#wuggy.evaluators.ld1nn.ld1nn">ld1nn</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>